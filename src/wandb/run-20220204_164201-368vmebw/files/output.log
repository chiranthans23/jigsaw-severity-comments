







































































































































Epoch 0:  94%|█████████████████████████████████████████████████████████████████████████▌    | 4140/4391 [37:34<02:16,  1.84it/s, loss=0.00699, v_num=mebw, train_loss=0.000429]









Validating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 259/259 [02:21<00:00,  1.83it/s]












































































































































Epoch 1:  94%|████████████████████████████████████████████████████████▌   | 4140/4391 [37:27<02:16,  1.84it/s, loss=0.00662, v_num=mebw, train_loss=0.000383, val_loss=0.00634]








Validating:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉         | 240/259 [02:11<00:10,  1.83it/s]











































































































































Epoch 2:  94%|███████████████████████████████████████████████████████████▍   | 4140/4391 [37:20<02:15,  1.85it/s, loss=0.00644, v_num=mebw, train_loss=0.003, val_loss=0.00634]









Validating:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉         | 240/259 [02:10<00:10,  1.83it/s]









































































































































Epoch 3:  94%|██████████████████████▋ | 4140/4391 [37:22<02:15,  1.85it/s, loss=0.00701, v_num=mebw, train_loss=0.0221, val_loss=0.00636]








Validating:  93%|███████████████████████████████████████████████████████████████████████████████▋      | 240/259 [02:10<00:10,  1.83it/s]

Epoch 3, global step 16527: val_loss was not in top 1
[33m====== Fold: 3 ======
Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Some weights of the model checkpoint at ../model/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
  | Name       | Type         | Params
--------------------------------------------
0 | criterion  | MSELoss      | 0
1 | model      | RobertaModel | 124 M
2 | layer_norm | LayerNorm    | 1.5 K
3 | dropout    | Dropout      | 0
4 | dense      | Sequential   | 197 K
--------------------------------------------
124 M     Trainable params
0         Non-trainable params
124 M     Total params
249.689   Total estimated model params size (MB)
Traceback (most recent call last):
  File "train_rberta.py", line 268, in <module>
    trainer.fit(model, data_module)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 741, in fit
    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1199, in _run
    self._dispatch()
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1279, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1289, in run_stage
    return self._run_train()
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1311, in _run_train
    self._run_sanity_check(self.lightning_module)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1375, in _run_sanity_check
    self._evaluation_loop.run()
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 110, in advance
    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/base.py", line 140, in run
    self.on_run_start(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 86, in on_run_start
    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/utilities.py", line 121, in _update_dataloader_iter
    dataloader_iter = enumerate(data_fetcher, batch_idx)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/fetching.py", line 197, in __iter__
    self.dataloader_iter = iter(self.dataloader)
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 359, in __iter__
    return self._get_iterator()
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 305, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 918, in __init__
    w.start()
  File "/opt/conda/lib/python3.7/multiprocessing/process.py", line 112, in start
    self._popen = self._Popen(self)
  File "/opt/conda/lib/python3.7/multiprocessing/context.py", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/opt/conda/lib/python3.7/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/opt/conda/lib/python3.7/multiprocessing/popen_fork.py", line 20, in __init__
    self._launch(process_obj)
  File "/opt/conda/lib/python3.7/multiprocessing/popen_fork.py", line 70, in _launch
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory
Validation sanity check:   0%|                                                                                     | 0/2 [00:00<?, ?it/s]