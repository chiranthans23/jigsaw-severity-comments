


































































































































Epoch 0:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎              | 3990/4488 [1:00:16<07:31,  1.10it/s, loss=0.224, v_num=4and, train_loss=-]
















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:50<00:11,  1.67it/s]






































































































































Epoch 1:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎            | 3990/4488 [59:46<07:27,  1.11it/s, loss=0.277, v_num=4and, train_loss=0.651, val_loss=0.281]

















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:51<00:11,  1.63it/s]




































































































































Epoch 2:  89%|████████████████████████████████████████████████████████████████████████████████████████████████████████▉             | 3990/4488 [59:56<07:28,  1.11it/s, loss=0.238, v_num=4and, train_loss=-, val_loss=0.283]
















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:50<00:11,  1.66it/s]

[33m====== Fold: 1 ======
Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Some weights of the model checkpoint at ../model/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
  | Name      | Type             | Params
-----------------------------------------------
0 | criterion | CrossEntropyLoss | 0
1 | model     | RobertaModel     | 124 M
2 | conv      | Conv2d           | 389 K
3 | relu      | ReLU             | 0
4 | pool      | MaxPool2d        | 0
5 | dropout   | Dropout          | 0
6 | flat      | Flatten          | 0
7 | fc        | Linear           | 39.8 K
8 | softmax   | LogSoftmax       | 0
-----------------------------------------------
125 M     Trainable params
0         Non-trainable params
125 M     Total params
250.150   Total estimated model params size (MB)
Validation sanity check:   0%|                                                                                                                                                                          | 0/2 [00:00<?, ?it/s]





































































































































Epoch 0:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊              | 3990/4488 [1:00:10<07:30,  1.11it/s, loss=0.241, v_num=4and, train_loss=0.268]

















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:55<00:11,  1.61it/s]





































































































































Epoch 1:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 3990/4488 [1:00:05<07:29,  1.11it/s, loss=0.198, v_num=4and, train_loss=-, val_loss=0.286]

















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:56<00:11,  1.62it/s]





































































































































Epoch 2:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████▌            | 3990/4488 [1:00:05<07:30,  1.11it/s, loss=0.292, v_num=4and, train_loss=0.768, val_loss=0.282]
















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:54<00:11,  1.63it/s]


Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [1:05:25<00:00,  1.14it/s, loss=0.313, v_num=4and, train_loss=-, val_loss=0.280]
Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
[33m====== Fold: 2 ======
Some weights of the model checkpoint at ../model/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
  | Name      | Type             | Params
-----------------------------------------------
0 | criterion | CrossEntropyLoss | 0
1 | model     | RobertaModel     | 124 M
2 | conv      | Conv2d           | 389 K
3 | relu      | ReLU             | 0
4 | pool      | MaxPool2d        | 0
5 | dropout   | Dropout          | 0
6 | flat      | Flatten          | 0
7 | fc        | Linear           | 39.8 K
8 | softmax   | LogSoftmax       | 0
-----------------------------------------------
125 M     Trainable params
0         Non-trainable params
125 M     Total params
250.150   Total estimated model params size (MB)
Validation sanity check:   0%|                                                                                                                                                                          | 0/2 [00:00<?, ?it/s]





































































































































Epoch 0:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎              | 3990/4488 [1:00:13<07:31,  1.10it/s, loss=0.245, v_num=4and, train_loss=-]
















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:56<00:11,  1.62it/s]






































































































































Epoch 1:  89%|████████████████████████████████████████████████████████████████████████████████████████████████████████             | 3990/4488 [1:00:10<07:30,  1.11it/s, loss=0.28, v_num=4and, train_loss=-, val_loss=0.267]
















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:58<00:11,  1.62it/s]






































































































































Epoch 2:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████▌            | 3990/4488 [1:00:07<07:30,  1.11it/s, loss=0.296, v_num=4and, train_loss=0.320, val_loss=0.265]
















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:59<00:11,  1.60it/s]

Epoch 2, global step 11966: val_loss was not in top 1
Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
[33m====== Fold: 3 ======
Some weights of the model checkpoint at ../model/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
  | Name      | Type             | Params
-----------------------------------------------
0 | criterion | CrossEntropyLoss | 0
1 | model     | RobertaModel     | 124 M
2 | conv      | Conv2d           | 389 K
3 | relu      | ReLU             | 0
4 | pool      | MaxPool2d        | 0
5 | dropout   | Dropout          | 0
6 | flat      | Flatten          | 0
7 | fc        | Linear           | 39.8 K
8 | softmax   | LogSoftmax       | 0
-----------------------------------------------
125 M     Trainable params
0         Non-trainable params
125 M     Total params
250.150   Total estimated model params size (MB)
Epoch 0:   0%|                                                                                                                                                                                       | 0/4488 [00:00<?, ?it/s]




































































































































Epoch 0:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏              | 3990/4488 [1:00:20<07:31,  1.10it/s, loss=0.23, v_num=4and, train_loss=-]
















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:53<00:11,  1.63it/s]






































































































































Epoch 1:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████▌            | 3990/4488 [1:00:21<07:31,  1.10it/s, loss=0.245, v_num=4and, train_loss=0.379, val_loss=0.286]
















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:57<00:11,  1.62it/s]






































































































































Epoch 2:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 3990/4488 [1:00:09<07:30,  1.11it/s, loss=0.227, v_num=4and, train_loss=-, val_loss=0.277]

















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:54<00:11,  1.62it/s]
Epoch 2, global step 11966: val_loss was not in top 1
[33m====== Fold: 4 ======
Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Some weights of the model checkpoint at ../model/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
  | Name      | Type             | Params
-----------------------------------------------
0 | criterion | CrossEntropyLoss | 0
1 | model     | RobertaModel     | 124 M
2 | conv      | Conv2d           | 389 K
3 | relu      | ReLU             | 0
4 | pool      | MaxPool2d        | 0
5 | dropout   | Dropout          | 0
6 | flat      | Flatten          | 0
7 | fc        | Linear           | 39.8 K
8 | softmax   | LogSoftmax       | 0
-----------------------------------------------
125 M     Trainable params
0         Non-trainable params
125 M     Total params
250.150   Total estimated model params size (MB)
Validation sanity check:   0%|                                                                                                                                                                          | 0/2 [00:00<?, ?it/s]





































































































































Epoch 0:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎              | 3990/4488 [1:00:20<07:31,  1.10it/s, loss=0.259, v_num=4and, train_loss=-]

















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:53<00:11,  1.63it/s]





































































































































Epoch 1:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 3990/4488 [1:00:02<07:29,  1.11it/s, loss=0.238, v_num=4and, train_loss=-, val_loss=0.285]
















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:52<00:11,  1.65it/s]






































































































































Epoch 2:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 3990/4488 [1:00:07<07:30,  1.11it/s, loss=0.224, v_num=4and, train_loss=-, val_loss=0.281]

















Validating:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 480/499 [04:52<00:11,  1.63it/s]

Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4488/4488 [1:05:25<00:00,  1.14it/s, loss=0.268, v_num=4and, train_loss=1.190, val_loss=0.281]